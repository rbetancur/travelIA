# ‚öôÔ∏è Configuraci√≥n del Sistema - ViajeIA

Documentaci√≥n completa de variables de entorno, constantes del sistema y configuraci√≥n del modelo de IA.

## üìã Tabla de Contenidos

- [Variables de Entorno](#variables-de-entorno)
- [Constantes del Sistema](#constantes-del-sistema)
- [Configuraci√≥n del Modelo de IA](#configuraci√≥n-del-modelo-de-ia)
- [Estructura de Prompts Optimizados](#estructura-de-prompts-optimizados)
- [Optimizaci√≥n de Tokens](#optimizaci√≥n-de-tokens)
- [Configuraci√≥n de Servicios Externos](#configuraci√≥n-de-servicios-externos)

---

## Variables de Entorno

### ‚ö†Ô∏è Importante: No se usa archivo `.env`

**Este proyecto NO utiliza archivos `.env`** por razones de seguridad. Todas las configuraciones se realizan mediante **variables de entorno del sistema operativo**.

### Variables Requeridas

#### GEMINI_API_KEY (Obligatoria)

API key de Google Gemini para el procesamiento de IA.

- **Tipo**: String
- **Requerida**: ‚úÖ S√≠
- **Ubicaci√≥n**: Variable de entorno del sistema
- **Formato**: String alfanum√©rico (ej: `AIzaSy...`)
- **Obtenci√≥n**: [Google AI Studio](https://makersuite.google.com/app/apikey)

**Configuraci√≥n:**

```bash
# Linux/Mac
export GEMINI_API_KEY=tu_api_key_aqui

# Windows (PowerShell)
$env:GEMINI_API_KEY="tu_api_key_aqui"

# Windows (CMD)
set GEMINI_API_KEY=tu_api_key_aqui
```

**Verificaci√≥n:**

```bash
# Linux/Mac
echo $GEMINI_API_KEY

# Windows (PowerShell)
$env:GEMINI_API_KEY

# Windows (CMD)
echo %GEMINI_API_KEY%
```

**Uso en el c√≥digo:**

```python
# backend/main.py
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    print("‚ö†Ô∏è  ADVERTENCIA: GEMINI_API_KEY no encontrada")
else:
    genai.configure(api_key=GEMINI_API_KEY)
```

### Variables Opcionales

#### GEMINI_MODEL

Modelo de Gemini a utilizar. Por defecto usa `gemini-2.0-flash` (gratuito).

- **Tipo**: String
- **Requerida**: ‚ùå No
- **Valor por defecto**: `"gemini-2.0-flash"`
- **Valores permitidos**:
  - `gemini-2.0-flash` (recomendado, gratuito)
  - `gemini-2.5-flash` (gratuito)
  - `gemini-2.0-flash-lite` (gratuito)
  - `gemini-flash-latest` (gratuito)
  - `gemini-pro-latest` (gratuito con l√≠mites)

**Configuraci√≥n:**

```bash
# Linux/Mac
export GEMINI_MODEL=gemini-2.0-flash

# Windows (PowerShell)
$env:GEMINI_MODEL="gemini-2.0-flash"
```

**Uso en el c√≥digo:**

```python
# backend/main.py
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.0-flash")
model = genai.GenerativeModel(GEMINI_MODEL)
```

**Validaci√≥n:**

El sistema valida que solo se usen modelos gratuitos (Flash). Si se intenta usar un modelo Pro de pago, se lanza un error 400.

#### OPENWEATHER_API_KEY (Opcional)

API key de OpenWeatherMap para informaci√≥n del clima.

- **Tipo**: String
- **Requerida**: ‚ùå No (opcional, pero recomendada)
- **Obtenci√≥n**: [OpenWeatherMap API](https://openweathermap.org/api)
- **Plan gratuito**: 60 llamadas/minuto, 1M llamadas/mes

**Configuraci√≥n:**

```bash
# Linux/Mac
export OPENWEATHER_API_KEY=tu_api_key_aqui

# Windows (PowerShell)
$env:OPENWEATHER_API_KEY="tu_api_key_aqui"
```

**Uso en el c√≥digo:**

```python
# backend/weather.py
self.api_key = api_key or os.getenv("OPENWEATHER_API_KEY")
```

#### UNSPLASH_API_KEY (Opcional)

API key de Unsplash para fotos de destinos.

- **Tipo**: String
- **Requerida**: ‚ùå No (opcional, pero recomendada)
- **Obtenci√≥n**: [Unsplash Developers](https://unsplash.com/developers)
- **Plan gratuito**: 50 solicitudes/hora

**Configuraci√≥n:**

```bash
# Linux/Mac
export UNSPLASH_API_KEY=tu_api_key_aqui

# Windows (PowerShell)
$env:UNSPLASH_API_KEY="tu_api_key_aqui"
```

**Uso en el c√≥digo:**

```python
# backend/unsplash.py
self.api_key = api_key or os.getenv("UNSPLASH_API_KEY")
```

#### ALLOWED_ORIGINS

Or√≠genes permitidos para CORS. Por defecto permite `http://localhost:3000`.

- **Tipo**: String (separado por comas)
- **Requerida**: ‚ùå No
- **Valor por defecto**: `"http://localhost:3000"`
- **Formato**: `"http://localhost:3000,https://example.com"`

**Configuraci√≥n:**

```bash
# Linux/Mac
export ALLOWED_ORIGINS="http://localhost:3000,https://viajeia.com"

# Windows (PowerShell)
$env:ALLOWED_ORIGINS="http://localhost:3000,https://viajeia.com"
```

**Uso en el c√≥digo:**

```python
# backend/main.py
allowed_origins = os.getenv("ALLOWED_ORIGINS", "http://localhost:3000").split(",")
```

#### ENVIRONMENT

Variable de entorno para indicar el ambiente (desarrollo/producci√≥n).

- **Tipo**: String
- **Requerida**: ‚ùå No
- **Valores**: `"production"` o cualquier otro valor (desarrollo)
- **Efecto**: Si es `"production"`, permite todos los or√≠genes en CORS (`["*"]`)

**Configuraci√≥n:**

```bash
# Linux/Mac
export ENVIRONMENT=production

# Windows (PowerShell)
$env:ENVIRONMENT="production"
```

**Uso en el c√≥digo:**

```python
# backend/main.py
if os.getenv("ENVIRONMENT") == "production":
    allowed_origins = ["*"]
```

#### PORT (Impl√≠cito)

Puerto en el que se ejecuta el servidor. No se configura directamente, sino que se pasa como argumento a Uvicorn.

- **Tipo**: Integer
- **Requerida**: ‚ùå No
- **Valor por defecto**: `8000`
- **Configuraci√≥n**: Se pasa como argumento al iniciar el servidor

**Inicio del servidor:**

```bash
# Puerto por defecto (8000)
uvicorn main:app --reload

# Puerto personalizado
uvicorn main:app --reload --port 8080
```

### Resumen de Variables de Entorno

| Variable | Requerida | Valor por Defecto | Descripci√≥n |
|----------|-----------|-------------------|-------------|
| `GEMINI_API_KEY` | ‚úÖ S√≠ | - | API key de Google Gemini |
| `GEMINI_MODEL` | ‚ùå No | `gemini-2.0-flash` | Modelo de Gemini a usar |
| `OPENWEATHER_API_KEY` | ‚ùå No | - | API key de OpenWeatherMap |
| `UNSPLASH_API_KEY` | ‚ùå No | - | API key de Unsplash |
| `ALLOWED_ORIGINS` | ‚ùå No | `http://localhost:3000` | Or√≠genes permitidos para CORS |
| `ENVIRONMENT` | ‚ùå No | - | Ambiente (production/desarrollo) |

---

## Constantes del Sistema

### Constantes de Validaci√≥n

Definidas en `backend/validators.py`:

#### MAX_QUESTION_LENGTH

Longitud m√°xima permitida para preguntas del usuario.

- **Valor**: `500` caracteres
- **Ubicaci√≥n**: `backend/validators.py`
- **Prop√≥sito**: Optimizaci√≥n de tokens y prevenci√≥n de abuso
- **Historial**: Reducido de 2000 caracteres para optimizar tokens

```python
# backend/validators.py
MAX_QUESTION_LENGTH = 500  # M√°ximo de caracteres permitidos en preguntas
```

**Comportamiento:**

- Si una pregunta excede 500 caracteres, se trunca autom√°ticamente
- El truncamiento es silencioso (no genera error)
- Se preservan palabras completas cuando es posible

#### MIN_QUESTION_LENGTH

Longitud m√≠nima requerida para preguntas del usuario.

- **Valor**: `10` caracteres
- **Ubicaci√≥n**: `backend/validators.py`
- **Prop√≥sito**: Evitar preguntas muy cortas o vac√≠as
- **Historial**: Aumentado de 1 car√°cter para mejorar calidad

```python
# backend/validators.py
MIN_QUESTION_LENGTH = 10   # M√≠nimo de caracteres requeridos en preguntas
```

**Comportamiento:**

- Si una pregunta tiene menos de 10 caracteres, se rechaza con error 422
- Mensaje de error: `"El campo 'question' debe tener al menos 10 caracteres"`

### Constantes de Otros Campos

#### MAX_DESTINATION_LENGTH

Longitud m√°xima para destinos.

- **Valor**: `200` caracteres
- **Ubicaci√≥n**: `backend/validators.py`
- **Uso**: Validaci√≥n de campo `destination`

#### MAX_SEARCH_QUERY_LENGTH

Longitud m√°xima para b√∫squedas de destinos.

- **Valor**: `100` caracteres
- **Ubicaci√≥n**: `backend/validators.py`
- **Uso**: Validaci√≥n de campo `query` en b√∫squedas

### Resumen de Constantes

| Constante | Valor | Ubicaci√≥n | Prop√≥sito |
|-----------|-------|------------|-----------|
| `MAX_QUESTION_LENGTH` | `500` | `validators.py` | Longitud m√°xima de preguntas |
| `MIN_QUESTION_LENGTH` | `10` | `validators.py` | Longitud m√≠nima de preguntas |
| `MAX_DESTINATION_LENGTH` | `200` | `validators.py` | Longitud m√°xima de destinos |
| `MAX_SEARCH_QUERY_LENGTH` | `100` | `validators.py` | Longitud m√°xima de b√∫squedas |

---

## Configuraci√≥n del Modelo de IA

### Modelo Principal: Gemini 2.0 Flash

El sistema utiliza **Google Gemini 2.0 Flash** como modelo de IA principal.

#### Caracter√≠sticas del Modelo

- **Nombre**: `gemini-2.0-flash`
- **Tipo**: Modelo Flash (gratuito)
- **Velocidad**: Optimizado para respuestas r√°pidas
- **Costo**: Gratuito (sin costos asociados)
- **Capacidad**: Generaci√≥n de respuestas sobre viajes

#### Configuraci√≥n en el C√≥digo

```python
# backend/main.py
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.0-flash")

# Validar que solo se usen modelos gratuitos
model_lower = GEMINI_MODEL.lower()
is_free_model = (
    "flash" in model_lower or 
    model_lower == "gemini-pro-latest" or
    model_lower == "models/gemini-pro-latest"
)

if not is_free_model:
    raise HTTPException(
        status_code=400,
        detail="Solo se permiten modelos GRATUITOS de Gemini"
    )

model = genai.GenerativeModel(GEMINI_MODEL)
```

#### Modelos Permitidos

El sistema valida que solo se usen modelos gratuitos:

| Modelo | Tipo | Estado |
|--------|------|--------|
| `gemini-2.0-flash` | Flash | ‚úÖ Permitido (por defecto) |
| `gemini-2.5-flash` | Flash | ‚úÖ Permitido |
| `gemini-2.0-flash-lite` | Flash | ‚úÖ Permitido |
| `gemini-flash-latest` | Flash | ‚úÖ Permitido |
| `gemini-pro-latest` | Pro (gratuito con l√≠mites) | ‚úÖ Permitido |
| `gemini-2.5-pro` | Pro (pago) | ‚ùå Rechazado |
| `gemini-2.0-pro` | Pro (pago) | ‚ùå Rechazado |

---

## Estructura de Prompts Optimizados

### System Prompt Reutilizable

El sistema utiliza un **system prompt centralizado** que se reutiliza en todos los prompts.

#### Ubicaci√≥n

- **Archivo**: `backend/prompts/system_prompt.txt`
- **Tama√±o**: ~31 tokens (123 caracteres)
- **Formato**: Texto plano

#### Contenido

```
Mary | consultora viajes | sofisticada | experta
Tono: elegante, refinado, discreto
Enfoque: excelencia, mejores opciones
```

#### Carga en el C√≥digo

```python
# backend/prompts/__init__.py
def load_system_prompt() -> str:
    """Carga el prompt de sistema reutilizable con cache."""
    global _system_prompt_cache
    
    if _system_prompt_cache is None:
        system_prompt_path = PROMPTS_DIR / "system_prompt.txt"
        with open(system_prompt_path, 'r', encoding='utf-8') as f:
            _system_prompt_cache = f.read().strip()
    
    return _system_prompt_cache
```

### Tipos de Prompts

#### 1. Prompt Estructurado

Usado para respuestas en formato JSON con 5 secciones.

**Archivo**: `backend/prompts/travel_planning_optimized.txt`

**Estructura:**

```
{system_prompt}

Responde en JSON con 5 secciones: alojamiento, comida_local, lugares_imperdibles, consejos_locales, estimacion_costos.
Cada secci√≥n: array de strings con recomendaciones detalladas (m√≠nimo 3-5).

Pregunta: {question}
```

**Tokens aproximados:**
- System prompt: ~31 tokens
- Instrucciones: ~35 tokens
- Pregunta: ~125 tokens (promedio)
- **Total**: ~191 tokens

**Uso:**

```python
# Se usa cuando hay destino establecido (formulario inicial o cambio expl√≠cito)
prompt = build_optimized_prompt(
    question=query.question,
    prompt_type="structured",
    destination=current_destination
)
```

#### 2. Prompt Contextual

Usado para respuestas conversacionales directas.

**Archivo**: `backend/prompts/travel_contextual_optimized.txt`

**Estructura:**

```
{system_prompt}

Responde de forma conversacional y directa (NO JSON), 2-4 p√°rrafos{destination_text}.

Pregunta: {question}
```

**Tokens aproximados:**
- System prompt: ~31 tokens
- Instrucciones: ~25 tokens
- Pregunta: ~125 tokens (promedio)
- **Total**: ~181 tokens

**Uso:**

```python
# Se usa para preguntas de seguimiento sobre el mismo destino
prompt = build_optimized_prompt(
    question=query.question,
    prompt_type="contextual",
    destination=current_destination
)
```

### Construcci√≥n de Prompts

La funci√≥n `build_optimized_prompt()` construye prompts ultra-optimizados:

```python
# backend/prompts/__init__.py
def build_optimized_prompt(
    question: str, 
    prompt_type: str = "structured", 
    destination: Optional[str] = None
) -> str:
    """
    Construye un prompt ultra-optimizado usando templates desde archivos.
    """
    # 1. Validar y limpiar la pregunta
    validate_input_length(question, "question", 
                         min_length=MIN_QUESTION_LENGTH, 
                         max_length=MAX_QUESTION_LENGTH)
    cleaned_question = sanitize_user_input(question, max_length=MAX_QUESTION_LENGTH)
    
    # 2. Cargar template desde archivo seg√∫n tipo
    if prompt_type == "structured":
        prompt = load_prompt("travel_planning_optimized", question=cleaned_question)
    else:
        destination_text = f" sobre {destination}" if destination else ""
        prompt = load_prompt("travel_contextual_optimized", 
            question=cleaned_question,
            destination_text=destination_text
        )
    
    return prompt
```

---

## Optimizaci√≥n de Tokens

### Resumen de Optimizaciones

El sistema ha implementado optimizaciones que logran una **reducci√≥n total del 92.0% en tokens** comparado con la versi√≥n original.

### M√©tricas de Optimizaci√≥n

#### Antes de la Optimizaci√≥n

| Componente | Tokens |
|------------|--------|
| Prompt base | ~1,513 tokens |
| Construcci√≥n contexto | ~503 tokens |
| Pregunta (1000 caracteres) | ~250 tokens |
| **TOTAL** | **~2,266 tokens** |

#### Despu√©s de la Optimizaci√≥n

| Componente | Tokens |
|------------|--------|
| System prompt | ~31 tokens |
| Instrucciones | ~35 tokens |
| Pregunta (500 caracteres m√°ximo) | ~125 tokens |
| **TOTAL** | **~191 tokens** |

#### Ahorro Total

- **Reducci√≥n**: 2,085 tokens (de ~2,266 a ~181)
- **Porcentaje**: 92.0% de reducci√≥n
- **Ahorro por llamada**: 2,085 tokens

### Optimizaciones Implementadas

#### 1. System Prompt Reutilizable

**Antes:**
- Instrucciones del sistema duplicadas en cada prompt
- ~500 caracteres (~125 tokens) por prompt
- 8-10 reglas detalladas

**Despu√©s:**
- System prompt centralizado
- ~123 caracteres (~31 tokens)
- 6 reglas esenciales
- **Reducci√≥n**: 74.2% en tokens base

#### 2. Construcci√≥n Simplificada

**Antes:**
- Construcci√≥n TOON con contexto din√°mico verboso
- ~78-500 tokens adicionales por construcci√≥n
- ~55 l√≠neas de c√≥digo

**Despu√©s:**
- Construcci√≥n directa desde templates
- ~35 tokens de instrucciones
- ~10 l√≠neas de c√≥digo
- **Reducci√≥n**: 83.9% en construcci√≥n de prompt

#### 3. Validaci√≥n de Longitud

**Antes:**
- `MAX_QUESTION_LENGTH = 2000` caracteres
- Preguntas largas consum√≠an muchos tokens

**Despu√©s:**
- `MAX_QUESTION_LENGTH = 500` caracteres
- Truncamiento autom√°tico si excede
- **Reducci√≥n**: 75% en tokens de pregunta (para preguntas largas)

### Impacto en Costos

| Escenario | Tokens Ahorrados | Reducci√≥n de Costos |
|-----------|------------------|---------------------|
| **Estructurado - Promedio** | 1,210 tokens | 80.0% |
| **Contextualizado - Promedio** | 308 tokens | 58.0% |
| **Construcci√≥n - Promedio** | 422 tokens | 83.9% |
| **TOTAL COMBINADO** | **2,085 tokens** | **92.0%** |

### Estructura de Prompt Optimizada

#### Prompt Estructurado Optimizado

```
{system_prompt}                    # ~31 tokens

Responde en JSON con 5 secciones: alojamiento, comida_local, lugares_imperdibles, consejos_locales, estimacion_costos.
Cada secci√≥n: array de strings con recomendaciones detalladas (m√≠nimo 3-5).  # ~35 tokens

Pregunta: {question}                  # ~125 tokens (promedio)
```

**Total**: ~191 tokens

#### Prompt Contextual Optimizado

```
{system_prompt}                    # ~31 tokens

Responde de forma conversacional y directa (NO JSON), 2-4 p√°rrafos{destination_text}.  # ~25 tokens

Pregunta: {question}               # ~125 tokens (promedio)
```

**Total**: ~181 tokens

### Comparaci√≥n: Antes vs Despu√©s

| M√©trica | Antes | Despu√©s | Mejora |
|---------|-------|---------|--------|
| **Tokens por prompt** | ~2,266 | ~181 | 92.0% |
| **L√≠neas de c√≥digo** | ~55 | ~10 | 82% |
| **Tiempo de construcci√≥n** | ~50ms | ~5ms | 90% |
| **Costo por llamada** | Alto | M√≠nimo | 92% |

---

## Configuraci√≥n de Servicios Externos

### OpenWeatherMap

#### Configuraci√≥n

- **API Key**: Variable de entorno `OPENWEATHER_API_KEY`
- **Endpoint**: `https://api.openweathermap.org/data/2.5/weather`
- **Cache**: Implementado en `backend/weather_cache.py`
- **Validaci√≥n**: Se valida al iniciar el servidor

#### L√≠mites del Plan Gratuito

- 60 llamadas por minuto
- 1,000,000 llamadas por mes
- Datos del clima actual
- Pron√≥stico de 5 d√≠as

### Unsplash

#### Configuraci√≥n

- **API Key**: Variable de entorno `UNSPLASH_API_KEY`
- **Endpoint**: `https://api.unsplash.com/search/photos`
- **Validaci√≥n**: Se valida al iniciar el servidor

#### L√≠mites del Plan Gratuito

- 50 solicitudes por hora
- Acceso a b√∫squeda de fotos
- Fotos de alta calidad

---

## Ejemplo de Configuraci√≥n Completa

### Archivo de Configuraci√≥n (No existe, solo referencia)

Aunque el proyecto no usa archivos `.env`, aqu√≠ est√° un ejemplo de c√≥mo se ver√≠an las variables:

```bash
# .env (NO SE USA - Solo referencia)
# Este archivo NO existe en el proyecto por seguridad

# API Keys (Obligatorias)
GEMINI_API_KEY=AIzaSy...

# API Keys (Opcionales)
OPENWEATHER_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
UNSPLASH_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Configuraci√≥n del Modelo
GEMINI_MODEL=gemini-2.0-flash

# Configuraci√≥n de CORS
ALLOWED_ORIGINS=http://localhost:3000,https://viajeia.com
ENVIRONMENT=production
```

### Configuraci√≥n Real (Variables de Entorno del Sistema)

**Linux/Mac (zsh):**

```bash
# Agregar a ~/.zshrc
echo 'export GEMINI_API_KEY=tu_api_key_aqui' >> ~/.zshrc
echo 'export OPENWEATHER_API_KEY=tu_api_key_aqui' >> ~/.zshrc
echo 'export UNSPLASH_API_KEY=tu_api_key_aqui' >> ~/.zshrc
echo 'export GEMINI_MODEL=gemini-2.0-flash' >> ~/.zshrc
echo 'export ENVIRONMENT=production' >> ~/.zshrc

# Cargar en sesi√≥n actual
source ~/.zshrc
```

**Windows (PowerShell):**

```powershell
[System.Environment]::SetEnvironmentVariable('GEMINI_API_KEY', 'tu_api_key_aqui', 'User')
[System.Environment]::SetEnvironmentVariable('OPENWEATHER_API_KEY', 'tu_api_key_aqui', 'User')
[System.Environment]::SetEnvironmentVariable('UNSPLASH_API_KEY', 'tu_api_key_aqui', 'User')
[System.Environment]::SetEnvironmentVariable('GEMINI_MODEL', 'gemini-2.0-flash', 'User')
[System.Environment]::SetEnvironmentVariable('ENVIRONMENT', 'production', 'User')
```

---

## Verificaci√≥n de Configuraci√≥n

### Script de Verificaci√≥n

```bash
#!/bin/bash
# verify_config.sh

echo "üîç Verificando configuraci√≥n de ViajeIA..."
echo ""

# Verificar GEMINI_API_KEY
if [ -z "$GEMINI_API_KEY" ]; then
    echo "‚ùå GEMINI_API_KEY no configurada"
else
    echo "‚úÖ GEMINI_API_KEY configurada"
fi

# Verificar GEMINI_MODEL
GEMINI_MODEL=${GEMINI_MODEL:-"gemini-2.0-flash"}
echo "‚úÖ GEMINI_MODEL: $GEMINI_MODEL"

# Verificar OPENWEATHER_API_KEY
if [ -z "$OPENWEATHER_API_KEY" ]; then
    echo "‚ö†Ô∏è  OPENWEATHER_API_KEY no configurada (opcional)"
else
    echo "‚úÖ OPENWEATHER_API_KEY configurada"
fi

# Verificar UNSPLASH_API_KEY
if [ -z "$UNSPLASH_API_KEY" ]; then
    echo "‚ö†Ô∏è  UNSPLASH_API_KEY no configurada (opcional)"
else
    echo "‚úÖ UNSPLASH_API_KEY configurada"
fi

echo ""
echo "üìä Resumen de configuraci√≥n completado"
```

### Verificaci√≥n Manual

```bash
# Verificar todas las variables
echo "GEMINI_API_KEY: ${GEMINI_API_KEY:+‚úÖ Configurada}"
echo "GEMINI_MODEL: ${GEMINI_MODEL:-gemini-2.0-flash}"
echo "OPENWEATHER_API_KEY: ${OPENWEATHER_API_KEY:+‚úÖ Configurada}"
echo "UNSPLASH_API_KEY: ${UNSPLASH_API_KEY:+‚úÖ Configurada}"
echo "ENVIRONMENT: ${ENVIRONMENT:-development}"
```

---

## Referencias

- **Documentaci√≥n de API Keys**: Ver `SECRETS.md`
- **Optimizaci√≥n de Tokens**: Ver `entrega/ejercicio1/Resumen Ejecutivo.md`
- **Documentaci√≥n de API**: Ver `docs/API_DOCUMENTATION.md`
- **Arquitectura**: Ver `docs/ARQUITECTURA.md`

---

**√öltima actualizaci√≥n**: 2024-01-15

